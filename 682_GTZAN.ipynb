{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfb24MLR-W_K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfXEubN3qdhC"
      },
      "outputs": [],
      "source": [
        "GTZAN_DIR_PATH = '/content/drive/MyDrive/ColabNotebooks'\n",
        "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxI6e5Uafjqg"
      },
      "source": [
        "\n",
        "## Import Packages & Set Global Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XHsmNzF3xAk"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "from librosa.effects import pitch_shift\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "GTZAN_DIR_PATH = '/content/drive/MyDrive/CS 682/682 Final Project'\n",
        "GENRES_DIR = GTZAN_DIR_PATH + \"/genres\"\n",
        "MELSPEC_GENRES_DIR = GTZAN_DIR_PATH + \"/melspec_genres\"\n",
        "GENRES_TRAIN = GTZAN_DIR_PATH + \"/genres_train\"\n",
        "GENRES_TEST = GTZAN_DIR_PATH + \"/genres_test\"\n",
        "MODELS_DIR = GTZAN_DIR_PATH + \"/models\"\n",
        "\n",
        "if not os.path.exists(MODELS_DIR): os.makedirs(MODELS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKMs2kd3xfK"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gCEs9tUfW0g"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "'''\n",
        "def addNoiseAugmentation(waveform, noise_amplitude):\n",
        "  noise = torch.randn(waveform.shape)\n",
        "  noisy_waveform = waveform + noise * noise_amplitude\n",
        "  return noisy_waveform\n",
        "\n",
        "def pitchShiftAugmentation(waveform, sample_rate):\n",
        "  n_steps = (torch.rand(1) * 8 - 4).item() #n_steps in [-4, 4] uniform dist\n",
        "  return torch.from_numpy(pitch_shift(waveform.numpy(), sr=sample_rate, n_steps=n_steps))\n",
        "\n",
        "def wav_2_melspec(path, pitchShift, addNoise, noise_amplitude):\n",
        "  waveform, sample_rate = torchaudio.load(path, normalize=True)\n",
        "  waveforms = [(waveform, \"\")]\n",
        "\n",
        "  if addNoise:\n",
        "    waveforms.append((addNoiseAugmentation(waveform, noise_amplitude), \"addNoise\"))\n",
        "\n",
        "  if pitchShift:\n",
        "    waveforms.append((pitchShiftAugmentation(waveform, sample_rate), \"pitchShift\"))\n",
        "\n",
        "  transform = transforms.MelSpectrogram(sample_rate)\n",
        "  melspecs = []\n",
        "  for waveform in waveforms:\n",
        "    melspec = transform(waveform[0])[:, :, :3300]\n",
        "    melspecs.append((melspec, waveform[1])) #some files are smaller, so we cut at 3300\n",
        "\n",
        "  return melspecs\n",
        "\n",
        "def preprocess_melspecs(trainingPathEnding = \"\", pitchShift=False, addNoise=False, noise_amplitude = 0.001):\n",
        "  if len(trainingPathEnding) > 0:\n",
        "    trainingPathEnding = \"_\" + trainingPathEnding\n",
        "  for genre in os.listdir(GENRES_DIR):\n",
        "    #### IGNORE .DS_Store ####\n",
        "    genre_dir = os.path.join(GENRES_DIR,genre)\n",
        "    genre_train_dir = os.path.join(GENRES_TRAIN + trainingPathEnding, genre)\n",
        "    genre_test_dir = os.path.join(GENRES_TEST, genre)\n",
        "\n",
        "    #### ADD DIRECTORIES ####\n",
        "    if not os.path.isdir(genre_dir): continue\n",
        "\n",
        "    if not os.path.exists(genre_train_dir):\n",
        "      os.makedirs(genre_train_dir)\n",
        "\n",
        "    if not os.path.exists(genre_test_dir):\n",
        "      os.makedirs(genre_test_dir)\n",
        "\n",
        "    #### CREATE TEST AND TRAIN SETS ####\n",
        "    files = os.listdir(genre_dir)\n",
        "    num_test = int(len(files) * 0.3)\n",
        "    test_files = np.array([True] * num_test + [False] * (len(files) - num_test))\n",
        "    np.random.shuffle(test_files)\n",
        "\n",
        "    for i, file in enumerate(files):\n",
        "      if \".wav\" not in file: continue\n",
        "\n",
        "      #### GENERATE BASE MELSPEC & AUGMENTATION MELSPECS ####\n",
        "      file_path = os.path.join(genre_dir,file)\n",
        "      melspecs = wav_2_melspec(file_path, pitchShift=pitchShift, addNoise=addNoise, noise_amplitude=noise_amplitude)\n",
        "\n",
        "      #### SAVE MELSPECS ####\n",
        "      if test_files[i]:\n",
        "        output_path = os.path.join(genre_test_dir, os.path.basename(file_path)[:-4] + \"_\" + melspecs[0][1] + \".npy\")\n",
        "        np.save(output_path, melspecs[0][0].numpy())\n",
        "      else:\n",
        "        for melspec in melspecs:\n",
        "          output_path = os.path.join(genre_train_dir, os.path.basename(file_path)[:-4] + \"_\" + melspec[1] + \".npy\")\n",
        "          np.save(output_path, melspec[0].numpy())\n",
        "\n",
        "#TODO: Uncomment next line to create test and training melspectrograms\n",
        "preprocess_melspecs(trainingPathEnding=\"all\", pitchShift=True, addNoise=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIjLhOKj3BBu"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfQwqSAkYA7y"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MelspecDataset(Dataset):\n",
        "  '''\n",
        "  root_dir: directory containing melspec genre subdirectories\n",
        "  splits: number of images to generate from each spectrogram\n",
        "  split_ranges: start time and end time for each audio split\n",
        "  '''\n",
        "  def __init__(self, root_dir, num_splits=1, W=3300, num_mixup=0):\n",
        "    self.root_dir = root_dir\n",
        "    self.genres = os.listdir(self.root_dir)\n",
        "    self.file_paths = []\n",
        "    self.split_ranges = []\n",
        "    self.labels = []\n",
        "    self.num_splits = num_splits\n",
        "\n",
        "\n",
        "\n",
        "    for id, genre in enumerate(os.listdir(self.root_dir)):\n",
        "      genre_dir = os.path.join(root_dir, genre)\n",
        "      if not os.path.isdir(genre_dir): continue\n",
        "      mel_paths = [os.path.join(genre_dir, file) for file in os.listdir(genre_dir) if \".npy\" in file]\n",
        "      #repeat the path and labels for each split\n",
        "      split_mel_paths = [path for path in mel_paths for _ in range(num_splits)]\n",
        "      self.file_paths.extend(split_mel_paths)\n",
        "      self.labels.extend([torch.eye(len(self.genres))[id]] * len(split_mel_paths))\n",
        "      self.split_ranges.extend([(i*W//num_splits, (i+1)*W//num_splits) for i in range(num_splits)] * len(mel_paths))\n",
        "\n",
        "    self.mixup_indexes, self.mixup_labels, self.lams = self.setMixupPairings(num_mixup)\n",
        "\n",
        "\n",
        "  def setMixupPairings(self, num_mixup):\n",
        "    # Welcome to funky town!\n",
        "    '''\n",
        "    mixup_indexes: (index_to_file1, index_to_file2)\n",
        "    lam = mixed_factor\n",
        "      x' = lam*x_1 + (1-lam)x_2\n",
        "      y' = lam*y_1 + (1-lam)y_2\n",
        "    '''\n",
        "    mixup_indexes = []\n",
        "    mixup_labels = []\n",
        "    lams = []\n",
        "    for _ in range(num_mixup):\n",
        "      # alpha is value in (0, inf)\n",
        "      alpha = torch.rand(1).item()\n",
        "      # value in Beta(alpha, alpha)\n",
        "      lam = dist.Beta(alpha, alpha).sample().item()\n",
        "\n",
        "      rand_index1 = torch.randint(0, len(self.file_paths), (1,)).item()\n",
        "      rand_index2 = torch.randint(0, len(self.file_paths), (1,)).item()\n",
        "      mixup_indexes.append((rand_index1, rand_index2))\n",
        "      mixup_labels.append(lam * self.labels[rand_index1] + (1- lam) * self.labels[rand_index2])\n",
        "      lams.append(lam)\n",
        "    return mixup_indexes, mixup_labels, lams\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels) + len(self.mixup_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #This is a horrible way to do mixup, but that's just the way it goes sometimes :)\n",
        "    label = None\n",
        "    melspec = None\n",
        "\n",
        "    if idx < len(self.file_paths):\n",
        "      melspec_path = self.file_paths[idx]\n",
        "      melspec = torch.from_numpy(np.load(melspec_path)[:, :, self.split_ranges[idx][0] : self.split_ranges[idx][1]])\n",
        "      label = self.labels[idx]\n",
        "    else:\n",
        "    #We compute mixup\n",
        "      #wrap around idx for mixup indexes\n",
        "      idx -= len(self.file_paths)\n",
        "      #get\n",
        "      i1, i2 = self.mixup_indexes[idx]\n",
        "      melspec_path1 = self.file_paths[i1]\n",
        "      melspec_path2 = self.file_paths[i2]\n",
        "      melspec1 = torch.from_numpy(np.load(melspec_path1)[:, :, self.split_ranges[i1][0] : self.split_ranges[i1][1]])\n",
        "      melspec2 = torch.from_numpy(np.load(melspec_path2)[:, :, self.split_ranges[i2][0] : self.split_ranges[i2][1]])\n",
        "      lam = self.lams[idx]\n",
        "      melspec = lam * melspec1 + (1- lam) * melspec2\n",
        "      label = self.mixup_labels[idx]\n",
        "    return melspec, label\n",
        "\n",
        "H = 128\n",
        "W= 3300\n",
        "num_splits = 10\n",
        "d_train = MelspecDataset(GENRES_TRAIN + \"_all\", num_splits)\n",
        "# print(d_train[7001])\n",
        "d_test = MelspecDataset(GENRES_TEST, num_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI3JVdfM3Feq"
      },
      "source": [
        "## Implement ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rMlhmF3Mgr"
      },
      "outputs": [],
      "source": [
        "class ResNetMelspec(nn.Module):\n",
        "  def __init__(self, dataloader, num_genres, lr=0.001, weight_decay=1e-6, max_iterations=20):\n",
        "    super(ResNetMelspec, self).__init__()\n",
        "\n",
        "    self.resnet18 = models.resnet18()#, num_classes=len(d.genres))\n",
        "    self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_genres) #set output to # of genres\n",
        "    self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "    self.dataloader = dataloader\n",
        "    self.lr=lr\n",
        "    self.weight_decay=weight_decay\n",
        "    self.max_iterations = max_iterations\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.resnet18(x)\n",
        "\n",
        "  #takes in all splits of a specific MelSpectrogram, outputs class guess\n",
        "  def predict_full_class(self, x):\n",
        "    predictions = self.predict(x)\n",
        "    max_values, _ = torch.max(predictions, dim=1, keepdim=True)\n",
        "    max_values = torch.eq(max_values, predictions).int()\n",
        "    # print(\"max_values:\", max_values)\n",
        "    max_class = torch.argmax(torch.sum(max_values, dim=0))\n",
        "    return max_class\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "    for i in range(self.max_iterations):\n",
        "      loss_sum = 0\n",
        "      # testing = 0\n",
        "      for X, labels in self.dataloader:\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predictions = self.predict(X)\n",
        "        loss = self.loss(predictions, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss\n",
        "        print(\"Loss:\", loss)\n",
        "\n",
        "      print(\"Epoch:\", i)\n",
        "      print(\"Loss:\", loss_sum)\n",
        "      self.save(\"R2Epoch\" + str(i))\n",
        "\n",
        "\n",
        "  def save(self, name):\n",
        "    torch.save({'model_state_dict': self.state_dict()}, MODELS_DIR + \"/\"  + name + \".pt\")\n",
        "\n",
        "\n",
        "  def load(self, name):\n",
        "    print(MODELS_DIR + \"/\" + name + \".pt\")\n",
        "    checkpoint = torch.load(MODELS_DIR + \"/\" + name + \".pt\")\n",
        "    self.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIdzgN0Nxjyi"
      },
      "source": [
        "### Initialize Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rIsRN5ixjCF"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(d_train, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(d_test, batch_size=num_splits, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtI4m5HrSoJy"
      },
      "source": [
        "### Train and Save ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6FC3UuFSncB"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataloader))\n",
        "resNetMelspec = ResNetMelspec(train_dataloader, num_genres=len(d_train.genres)).to(device)\n",
        "resNetMelspec.fit()\n",
        "resNetMelspec.save(\"ResNet_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kDLJ6RS5Hw"
      },
      "source": [
        "### Test ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfTcSSLpS9l5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "SAVE_FILE_NAME = \"Epoch8\" #Ex: \"Epoch8\"\n",
        "test_dataloader = DataLoader(d_test, batch_size=num_splits, shuffle=False)\n",
        "#LOAD FROM FILE\n",
        "resNetMelspecTest = ResNetMelspec(test_dataloader, num_genres=len(d_test.genres))\n",
        "resNetMelspecTest.load(SAVE_FILE_NAME)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "count = len(test_dataloader)\n",
        "with torch.no_grad():\n",
        "  for x, label in test_dataloader:\n",
        "    y_true.append(torch.argmax(label).item())\n",
        "    y_pred.append(resNetMelspecTest.predict_full_class(x).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEgXk7if5eHn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA9U6KxYvHZC"
      },
      "source": [
        "## Implement Paper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTwgTps9vbxc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Conv_2d_Wrapper(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.5):\n",
        "        super(Conv_2d_Wrapper, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=shape, padding=0) #shape//2)\n",
        "        #self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(pooling)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, wav):\n",
        "        out = self.conv(wav)\n",
        "        #out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class KCNN(nn.Module):\n",
        "    def __init__(self, dataloader,\n",
        "                       sample_rate=22050,\n",
        "                       n_fft=1024,\n",
        "                       f_min=0.0,\n",
        "                       f_max=11025.0,\n",
        "                       num_mels=128,\n",
        "                       num_classes=10, lr=0.001, weight_decay=1e-6, max_iterations=20):\n",
        "        super(KCNN, self).__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm2d(1)\n",
        "\n",
        "        # convolutional layers\n",
        "        self.layer1 = Conv_2d_Wrapper(1, 128)\n",
        "        self.layer2 = Conv_2d_Wrapper(128, 64)\n",
        "        self.layer3 = Conv_2d_Wrapper(64, 32)\n",
        "        self.layer4 = Conv_2d_Wrapper(32, 16)\n",
        "        self.layer5 = Conv_2d_Wrapper(16, 8)\n",
        "\n",
        "        # dense layers\n",
        "        self.dense1 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.dataloader = dataloader\n",
        "        self.lr=lr\n",
        "        self.weight_decay=weight_decay\n",
        "        self.max_iterations = max_iterations\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, mel):\n",
        "       # print(mel.shape)\n",
        "        # input batch normalization\n",
        "        #out = mel.unsqueeze(1)\n",
        "        # out = self.input_bn(out) # input normalization\n",
        "        # convolutional layers\n",
        "        out = self.layer1(mel)\n",
        "        #print(out.shape)\n",
        "        out = self.layer2(out)\n",
        "       # print(out.shape)\n",
        "        out = self.layer3(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer4(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer5(out)\n",
        "        #print(out.shape)\n",
        "        out = out.reshape(len(out), -1)\n",
        "        out = self.dense1(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "      return self.forward(x)\n",
        "\n",
        "    #takes in all splits of a specific MelSpectrogram, outputs class guess\n",
        "    def predict_full_class(self, x):\n",
        "      predictions = self.predict(x)\n",
        "      max_values, _ = torch.max(predictions, dim=1, keepdim=True)\n",
        "      max_values = torch.eq(max_values, predictions).int()\n",
        "      max_class = torch.argmax(torch.sum(max_values, dim=0))\n",
        "      return max_class\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "      for i in range(self.max_iterations):\n",
        "        loss_sum = 0\n",
        "        # testing = 0\n",
        "        for X, labels in self.dataloader:\n",
        "          X = X.to(device)\n",
        "          labels = labels.to(device)\n",
        "          predictions = self.predict(X)\n",
        "          loss = self.loss(predictions, labels)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # print(\"Loss:\", loss)\n",
        "          # if(testing %10 ==  0):\n",
        "          #   stop = input()\n",
        "          #   if stop == '1':\n",
        "          #     return\n",
        "          loss_sum += loss\n",
        "          print(\"Loss:\", loss)\n",
        "\n",
        "        print(\"Epoch:\", i)\n",
        "        print(\"Loss:\", loss_sum)\n",
        "        self.save(\"R2Epoch\" + str(i))\n",
        "\n",
        "\n",
        "    def save(self, name):\n",
        "      torch.save({'model_state_dict': self.state_dict()}, MODELS_DIR + \"/\"  + name + \".pt\")\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "      print(MODELS_DIR + \"/\" + name + \".pt\")\n",
        "      checkpoint = torch.load(MODELS_DIR + \"/\" + name + \".pt\")\n",
        "      self.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFu5roZYy9y3"
      },
      "source": [
        "### Initialize Dataloaders\n",
        "\n",
        "(Repeated from above for the sake of convenience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gey5r6Ivy5b3"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(d_train, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(d_test, batch_size=num_splits, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjqVE_2fviAH"
      },
      "source": [
        "### Load and Train Paper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV43i_KkvgT2"
      },
      "outputs": [],
      "source": [
        "kcnn = KCNN(train_dataloader).to(device)\n",
        "kcnn.fit()\n",
        "kcnn.save(\"KCNN_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ-tNj21v9VA"
      },
      "source": [
        "### Test Paper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc4Hb8sCv79V"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "SAVE_FILE_NAME = \"R2Epoch9\" #Ex: \"Epoch8\"\n",
        "test_dataloader = DataLoader(d_test, batch_size=num_splits, shuffle=False)\n",
        "#LOAD FROM FILE\n",
        "kcnn = KCNN(train_dataloader)\n",
        "kcnn.load(SAVE_FILE_NAME)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "count = len(test_dataloader)\n",
        "with torch.no_grad():\n",
        "  for x, label in test_dataloader:\n",
        "    y_true.append(torch.argmax(label).item())\n",
        "    y_pred.append(kcnn.predict_full_class(x).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyTaFf6rMG2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}